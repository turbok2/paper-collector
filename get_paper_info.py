import os
import json
import yaml
import pandas as pd
from dotenv import load_dotenv
from LLM_MODEL import LocalApi, GPTApi
import requests  # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸


# --- PDF ë¶„ì„ ì„œë¹„ìŠ¤ í˜¸ì¶œ í•¨ìˆ˜ ---
def get_pdf_json(pdf_path, service_url, request_timeout):
    """
    ì§€ì •ëœ PDF íŒŒì¼ì„ ì™¸ë¶€ ì„œë¹„ìŠ¤ë¡œ ì „ì†¡í•˜ì—¬ JSON ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ìŠµë‹ˆë‹¤.

    Args:
        pdf_path (str): ë¶„ì„í•  PDF íŒŒì¼ì˜ ë¡œì»¬ ê²½ë¡œ.
        service_url (str): PDF ë¶„ì„ ì„œë¹„ìŠ¤ì˜ URL.
        request_timeout (int): ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ).

    Returns:
        dict or None: ì„œë¹„ìŠ¤ì—ì„œ ë°˜í™˜ëœ JSON ë°ì´í„° (ì„±ê³µ ì‹œ), ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ).
    """
    error = ""
    try:
        with open(pdf_path, "rb") as f:
            files = {"file": (os.path.basename(pdf_path), f, "application/pdf")}
            # st.info(f"PDF ë¶„ì„ ì„œë¹„ìŠ¤ ({service_url})ë¡œ íŒŒì¼ì„ ì „ì†¡ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...")
            response = requests.post(service_url, files=files, timeout=request_timeout)

        response.raise_for_status()  # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        return response.json(), error
    except requests.exceptions.Timeout:
        error = f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼: PDF ë¶„ì„ ì„œë¹„ìŠ¤ê°€ {request_timeout}ì´ˆ ë‚´ì— ì‘ë‹µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return None, error
    except requests.exceptions.ConnectionError:
        error = "ì—°ê²° ì˜¤ë¥˜: PDF ë¶„ì„ ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë¹„ìŠ¤ URLì„ í™•ì¸í•˜ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        return None, error
    except requests.exceptions.RequestException as e:
        error = f"PDF ë¶„ì„ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        return None, error
    except json.JSONDecodeError:
        error = "ì„œë¹„ìŠ¤ì—ì„œ ìœ íš¨í•œ JSON ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        return None, error
    except Exception as e:
        error = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"
        return None, error


DEBUG = False

PROMPT_PATH = r"Active_prompts_TOTAL.yaml"

LLAMA_URL = "http://10.91.200.20:11437"

MODEL_NAME = "gpt-4o-2024-08-06"  # gpt-4o-2024-11-20
# MODEL_NAME = "gpt-4o-2024-11-20"

# MODEL_NAME = "llama3.1:70b"
# MODEL_NAME = "llama3.1:8b"


# ì‹¤í–‰ íŒŒë¼ë¯¸í„°
ALLOWED_TYPES = None  # ì˜ˆ: ["Text", "Title"] ë˜ëŠ” None

# TARGET_PAGES = [2] # ì›í•˜ëŠ” í˜ì´ì§€ ë³„ë„ ì…ë ¥
TARGET_PAGES = None

MAX_PAGE_NUMBER = 2  # ì—†ìœ¼ë©´, 1~2 í˜ì´ì§€ ìë™ ì„ íƒ

# [STEP 1] API KEY loading
load_dotenv(override=True)  # modify
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒğŸ”‘âŒ 'OPENAI_API_KEY'ê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
# if DEBUG: print("âœ… API KEY LOADED :", bool(OPENAI_API_KEY))


# [STEP 2] ACTIVE_PROMPT loading
def load_prompts(yaml_path):
    with open(yaml_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


# [STEP 3] JSON TEXT loading (HURIDOCS)
# INPUTìš© JSON ì „ì²˜ë¦¬ í•¨ìˆ˜
def extract_text_from_json_blocks(
    json_data,
    allowed_types=None,
    target_pages=None,
    max_page_number=2,
    max_preview_length=3000,
):

    if not isinstance(json_data, list):
        raise ValueError("Expected json_data to be a list of blocks")

    # Determine unique types in data for reference
    all_types = sorted(set(block.get("type", "Unknown") for block in json_data))

    filtered_text_blocks = []
    for block in json_data:
        page = block.get("page_number")
        text = block.get("text", "")
        block_type = block.get("type", "Unknown")

        if not isinstance(page, int) or not isinstance(text, str):
            continue  # skip invalid entries

        # íƒ€ê²Ÿ í˜ì´ì§€ ë²ˆí˜¸ í•„í„°ë§
        if target_pages is not None:
            if page not in target_pages:
                continue

        elif max_page_number is not None:
            if page > max_page_number:
                continue  # skip pages beyond the limit

        if allowed_types is not None and block_type not in allowed_types:
            continue  # skip types not in the allowed list

        filtered_text_blocks.append(text)

    combined_text = "\n".join(filtered_text_blocks)

    # âœ… í„°ë¯¸ë„ ì¶œë ¥ (ì§€ì •ëœ ê¸¸ì´ê¹Œì§€ë§Œ ë¯¸ë¦¬ë³´ê¸°)
    if DEBUG:
        print("-" * 80)
        print("\nğŸ“„ğŸ“„ [FILTERED EXTRACTED TEXT] ğŸ“„ğŸ“„\n")
        preview = combined_text[:max_preview_length]
        print(
            preview
            + ("\n... (truncated)" if len(combined_text) > max_preview_length else "")
        )
        print("-" * 80)

    # print(f"âœ… Found {len(filtered_text_blocks)} text blocks from pages â‰¤ {max_page_number}")
    # print(f"âœ… Types available in data: {all_types}")

    return combined_text, all_types


# [STEP 4] PROMPTì˜ ê° field ì½ê³ , descriptionì— ë”°ë¼ ì‘ì—… ìˆ˜í–‰ ì •ì˜
def process_file(file_path, prompts, llm):
    with open(file_path, "r", encoding="utf-8") as f:
        json_data = json.load(f)

    combined_text, all_types = extract_text_from_json_blocks(
        json_data,
        allowed_types=ALLOWED_TYPES,
        target_pages=TARGET_PAGES,
        max_page_number=MAX_PAGE_NUMBER,
    )
    result = {}

    # âœ… í•œ ê¸€ìë¼ë„ í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ NO_TEXT ì²˜ë¦¬
    if combined_text.strip() == "":
        for field in prompts.keys():
            result[field] = "NO_TEXT"
            result[f"RAW_{field}"] = "NO_TEXT"
        return result

    for field, field_info in prompts.items():
        instruction = field_info.get("description", "")
        # if DEBUG: print(f"[INFO] Extracting '{field}'...")

        res = llm.send_request(field, instruction, combined_text)
        parsed = res["parsed"]
        # raw = res["raw"]
        # status = "success" if parsed != "ERROR" else "error"

        result[field] = parsed
        # result[f"RAW_{field}"] = raw

    return result


# def parsing_json(json_data):
#     c_df = pd.DataFrame(list(json_data.items()), columns=['Key', 'Value'])
#     authors = json_data["AUTHOR_LIST"].split("; ")
#     affiliations = json_data["AUTHOR_AFFILIATION"]["AFFILIATION"]
#     print("authors:\n",authors)
#     print("affiliations:\n",affiliations)
#     # ì €ìì™€ ì†Œì†ì„ ë§¤ì¹­ (ì €ì ìˆ˜ë§Œí¼ë§Œ)
#     structured_data = []
#     for i, author in enumerate(authors):
#         structured_data.append({
#             'Title': json_data["TITLE"],
#             'Author': author,
#             'Affiliation': affiliations[i] if i < len(affiliations) else 'N/A'
#         })

#     a_df = pd.DataFrame(structured_data)
#     return a_df, c_df


def parsing_json(json_data):
    # [STEP 1] C_DATA êµ¬ì„±
    DESIRED_KEYS = [
        "TITLE",
        "AUTHOR_LIST",
        "AFFILIATION_LIST",
        "FIRST_AUTHOR",
        "CORRESPONDING_AUTHOR",
        "KEYWORDS",
        "DATE_METADATA",
        "BIBLIOGRAPHY_INFORMATION",
        "JOURNAL_NAME",
        "PUBLICATION_YEAR",
        "VOLUME",
        "ISSUE",
        "PAGE",
        "DOI",
    ]

    filtered_data = {k: json_data.get(k, "") for k in DESIRED_KEYS}
    filtered_data["JSON_FILE_NAME"] = "json_input"  # ë˜ëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì„ ìˆ˜ ìˆìŒ

    # CO_AUTHOR ì²˜ë¦¬
    all_authors = set(
        name.strip()
        for name in json_data.get("AUTHOR_LIST", "").split(";")
        if name.strip()
    )
    first_authors = set(
        name.strip()
        for name in json_data.get("FIRST_AUTHOR", "").split(";")
        if name.strip()
    )
    corresponding_authors = set(
        name.strip()
        for name in json_data.get("CORRESPONDING_AUTHOR", "").split(";")
        if name.strip()
    )
    co_authors = all_authors - first_authors - corresponding_authors
    filtered_data["CO_AUTHOR"] = "; ".join(sorted(co_authors))

    C_DATA = pd.DataFrame([filtered_data])

    # [STEP 2] AUTHOR_ROLE ìƒì„±
    ROLE_PRIORITY = {"FIRST_AUTHOR": 1, "CORRESPONDING_AUTHOR": 2, "CO_AUTHOR": 3}
    AUTHOR_ROLE_MAP = {}

    def add_roles(names_str, role, remove_co=False):
        for name in (n.strip() for n in names_str.split(";") if n.strip()):
            AUTHOR_ROLE_MAP.setdefault(name, set())
            if remove_co:
                AUTHOR_ROLE_MAP[name].discard("CO_AUTHOR")
            AUTHOR_ROLE_MAP[name].add(role)

    add_roles(json_data.get("AUTHOR_LIST", ""), "CO_AUTHOR")
    add_roles(json_data.get("FIRST_AUTHOR", ""), "FIRST_AUTHOR", remove_co=True)
    add_roles(
        json_data.get("CORRESPONDING_AUTHOR", ""),
        "CORRESPONDING_AUTHOR",
        remove_co=True,
    )

    AUTHOR_ROLE_DATA = pd.DataFrame(
        [
            {
                "JSON_FILE_NAME": "json_input",
                "AUTHOR": name,
                "ROLE": "; ".join(
                    sorted(roles, key=lambda r: ROLE_PRIORITY.get(r, 99))
                ),
            }
            for name, roles in AUTHOR_ROLE_MAP.items()
        ]
    )

    # [STEP 3] AUTHOR_AFFILIATION
    AUTHOR_AFFILIATIONS = []
    for entry in json_data.get("AUTHOR_AFFILIATION", []):
        author = entry.get("AUTHOR", "").strip()
        aff_text = entry.get("AFFILIATION", "").strip()
        if aff_text:
            for aff in [a.strip() for a in aff_text.split(";") if a.strip()]:
                AUTHOR_AFFILIATIONS.append(
                    {
                        "JSON_FILE_NAME": "json_input",
                        "AUTHOR": author,
                        "AFFILIATION": aff,
                    }
                )

    AUTHOR_AFFILIATION_DATA = pd.DataFrame(AUTHOR_AFFILIATIONS)

    # [STEP 4] A_DATA ë³‘í•©
    A_DATA = pd.merge(
        AUTHOR_AFFILIATION_DATA,
        AUTHOR_ROLE_DATA,
        on=["JSON_FILE_NAME", "AUTHOR"],
        how="outer",
    )
    # print("AUTHOR_AFFILIATION_DATA:\n",AUTHOR_AFFILIATION_DATA)
    # print("AUTHOR_ROLE_DATA:\n", AUTHOR_ROLE_DATA)
    cdf = C_DATA.T.reset_index()
    cdf.columns = ["Key", "Value"]
    # dict, list ë“± ë³µì¡í•œ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    cdf["Value"] = cdf["Value"].apply(
        lambda x: (
            json.dumps(x, ensure_ascii=False) if isinstance(x, (dict, list)) else x
        )
    )

    return A_DATA, cdf


def count_no_text(json_data):
    cnt_total = len(json_data)
    no_cnt = sum(1 for v in json_data.values() if v == "NO_TEXT")
    return cnt_total, no_cnt


def get_paper_df(filename):
    prompts = load_prompts(PROMPT_PATH)

    # âœ… ëª¨ë¸ ì„ íƒ
    if "gpt" in MODEL_NAME:
        print("gpt")
        llm = GPTApi(model_name=MODEL_NAME, api_key=OPENAI_API_KEY)
    else:
        print("local")
        llm = LocalApi(model_name=MODEL_NAME, base_url=LLAMA_URL)

    # if DEBUG: print(f"\n[ğŸ“„] Processing {filename} ...")

    json_data = process_file(filename, prompts, llm)
    # print("=====json_data:\n", json_data)
    cnt_total, no_cnt = count_no_text(json_data)
    print(f"ì „ì²´ í•­ëª© ìˆ˜: {cnt_total}, 'NO_TEXT'ì¸ í•­ëª© ìˆ˜: {no_cnt}")
    if no_cnt > 5:
        return None, None, no_cnt
    a_result, c_result = parsing_json(json_data)
    # print(c_result)
    # print(a_result)
    return json_data, a_result, c_result, no_cnt, MODEL_NAME
